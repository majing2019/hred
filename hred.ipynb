{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据准备（data/corpora_processed、data/conditions_index）\n",
    "\n",
    "构建单词索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '_pad_'), (1, '_unk_'), (2, '_start_'), (3, '_end_'), (4, '.'), (5, ','), (6, 'Hello'), (7, 'Oh'), (8, 'hi'), (9, '!')]\n",
      "[(0, 'neutral'), (1, 'joy'), (2, 'sadness')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "_tokenizer = nltk.tokenize.RegexpTokenizer(pattern='\\w+|[^\\w\\s]')\n",
    "\n",
    "VOCABULARY_MAX_SIZE = 50000\n",
    "MAX_CONDITIONS_NUM = 5\n",
    "\n",
    "dialogs = []\n",
    "tokens_counter = Counter()\n",
    "conditions_counter = Counter()\n",
    "tokenized_training_lines = []\n",
    "for line in open('data/corpora_processed/train_processed_dialogs.txt', 'r', encoding='utf-8'):\n",
    "    line_json = json.loads(line.strip())\n",
    "    dias = []\n",
    "    for entry in line_json:\n",
    "        tokens = _tokenizer.tokenize(entry['text'])\n",
    "        tokenized_training_lines.append(tokens)\n",
    "        dias.append({'text': ' '.join(tokens), 'condition': entry['condition']})\n",
    "        tokens_counter.update(tokens)\n",
    "        conditions_counter[entry['condition']] += 1\n",
    "    dialogs.append(dias)\n",
    "        \n",
    "# 构建vocab list\n",
    "special_tokens = ['_pad_', '_unk_', '_start_', '_end_']\n",
    "vocab = special_tokens + [token for token, _ in tokens_counter.most_common(VOCABULARY_MAX_SIZE - len(special_tokens))]\n",
    "\n",
    "# 构建condition list\n",
    "conditions = [condition for condition, _ in conditions_counter.most_common(MAX_CONDITIONS_NUM)]\n",
    "\n",
    "index_to_token = dict(enumerate(vocab))\n",
    "index_to_condition = dict(enumerate(conditions))\n",
    "\n",
    "with open('data/id2vocab', 'w', encoding='utf-8') as fh:\n",
    "        json.dump(index_to_token, fh, ensure_ascii=False)\n",
    "\n",
    "with open('data/id2condition', 'w', encoding='utf-8') as fh:\n",
    "        json.dump(index_to_condition, fh, ensure_ascii=False)\n",
    "        \n",
    "print(list(index_to_token.items())[:10])\n",
    "print(list(index_to_condition.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majing/workspace/learning/hred/conda-env/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Users/majing/workspace/learning/hred/conda-env/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "WORD_EMBEDDING_DIMENSION = 128\n",
    "W2V_WINDOW_SIZE = 10\n",
    "USE_SKIP_GRAM = True # 为False时用CBOW\n",
    "MIN_WORD_FREQ = 1\n",
    "_WORKERS_NUM = multiprocessing.cpu_count()\n",
    "\n",
    "word2vec_path = 'data/word2vec.bin'\n",
    "word2vec_model = Word2Vec(\n",
    "        window=W2V_WINDOW_SIZE,\n",
    "        size=WORD_EMBEDDING_DIMENSION,\n",
    "        max_vocab_size=VOCABULARY_MAX_SIZE,\n",
    "        min_count=MIN_WORD_FREQ,\n",
    "        workers=_WORKERS_NUM,\n",
    "        sg=USE_SKIP_GRAM)\n",
    "\n",
    "word2vec_model.build_vocab(tokenized_training_lines)\n",
    "word2vec_model.train(tokenized_training_lines, total_words=50000, epochs=10)\n",
    "word2vec_model.init_sims(replace=True) # 强制单位归一化，破坏性就地(打击非归一化向量), 更节省存储空间\n",
    "word2vec_model.save(word2vec_path, separately=[])\n",
    "\n",
    "# 在推理时可以用，导入词向量\n",
    "word2vec_model = Word2Vec.load(word2vec_path, mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入vocab和condition索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_pad_': 0, '_unk_': 1, '_start_': 2, '_end_': 3, '.': 4, ',': 5, 'Hello': 6, 'Oh': 7, 'hi': 8, '!': 9, ':': 10, ')': 11, 'How': 12, 'are': 13, 'you': 14, 'my': 15, 'friend': 16, '?': 17, 'Doing': 18, 'good': 19, 'Justin': 20, 'Bieber': 21, 'is': 22, 'the': 23, 'best': 24, 'Ok': 25}\n",
      "{'neutral': 0, 'joy': 1, 'sadness': 2}\n"
     ]
    }
   ],
   "source": [
    "with open('data/id2vocab', 'r', encoding='utf-8') as item_index_fh:\n",
    "        token_to_index = json.load(item_index_fh)\n",
    "        token_to_index = {v: int(k) for k, v in token_to_index.items()}\n",
    "\n",
    "\n",
    "with open('data/id2condition', 'r', encoding='utf-8') as item_index_fh:\n",
    "        condition_to_index = json.load(item_index_fh)\n",
    "        condition_to_index = {v: int(k) for k, v in condition_to_index.items()}\n",
    "        \n",
    "print (token_to_index)\n",
    "print (condition_to_index)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义Dataset和ModelParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Dataset = namedtuple('Dataset', ['x', 'y', 'condition_ids'])\n",
    "ModelParam = namedtuple('ModelParam', ['value', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint16), array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 7,  5,  8,  9, 10, 11, 12, 13, 14,  5, 15, 16, 17,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16), array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [20, 21, 22, 23, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16)]\n",
      "[array([ 2,  7,  5,  8,  9, 10, 11, 12, 13, 14,  5, 15, 16, 17,  3,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=uint16), array([ 2, 18, 19,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=uint16), array([ 2, 25,  4,  4,  4,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=uint16)]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "\n",
    "INPUT_CONTEXT_SIZE = 3\n",
    "INPUT_SEQUENCE_LENGTH = 30\n",
    "INPUT_SEQUENCE_LENGTH = 30\n",
    "OUTPUT_SEQUENCE_LENGTH = 32\n",
    "INTX = 'uint16'\n",
    "\n",
    "# 对数据进行分词等操作\n",
    "train_conditions = []\n",
    "tokenized_alternated_train_lines = []\n",
    "for dialog in dialogs:\n",
    "    for first_dialog_line, second_dialog_line in zip(dialog, dialog[1:]):\n",
    "        tokenized_alternated_train_lines.append(_tokenizer.tokenize(first_dialog_line['text']))\n",
    "        tokenized_alternated_train_lines.append(_tokenizer.tokenize(second_dialog_line['text']))\n",
    "        train_conditions.append(first_dialog_line['condition'])\n",
    "        train_conditions.append(second_dialog_line['condition'])\n",
    "\n",
    "# 数据进行X、Y的区分\n",
    "n_dialogs = sum(1 for _ in tokenized_alternated_train_lines)\n",
    "x_data_iterator_seq2seq = islice(tokenized_alternated_train_lines, 0, None, 2)\n",
    "y_data_iterator_seq2seq = islice(tokenized_alternated_train_lines, 1, None, 2)\n",
    "n_dialogs //= 2\n",
    "context = []\n",
    "x_data_iterator = [] # 训练的输入x\n",
    "y_data_iterator_for_context = [] # 训练的输出y\n",
    "last_y_line = None\n",
    "for x_line, y_line in zip(x_data_iterator_seq2seq, y_data_iterator_seq2seq):\n",
    "    if x_line != last_y_line:\n",
    "        context = []  # clear context if last response != current dialog context (new dialog)\n",
    "    context.append(x_line)\n",
    "    x_data_iterator.append(context[-INPUT_CONTEXT_SIZE:])  # yield list of tokenized lines\n",
    "    y_data_iterator_for_context.append(y_line)\n",
    "    last_y_line = y_line\n",
    "\n",
    "# X数据转成ID\n",
    "max_contexts_num = n_dialogs\n",
    "max_context_len = INPUT_CONTEXT_SIZE\n",
    "max_line_len = INPUT_SEQUENCE_LENGTH\n",
    "X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index['_pad_'], dtype=INTX)\n",
    "for context_idx, context in enumerate(x_data_iterator):\n",
    "    if context_idx >= max_contexts_num:\n",
    "        break\n",
    "\n",
    "    # take last max_content_len utterances\n",
    "    context = context[-max_context_len:]\n",
    "\n",
    "    # fill utterances to the end of context, keep first empty utterances padded.\n",
    "    utterance_offset = max_context_len - len(context)\n",
    "    for utterance_idx, utterance in enumerate(context):\n",
    "        for token_idx, token in enumerate(utterance[:max_line_len]):\n",
    "            X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n",
    "                if token in token_to_index else token_to_index[_unk_]\n",
    "                \n",
    "# Y数据转成ID\n",
    "max_lines_num = n_dialogs\n",
    "Y = np.full((max_lines_num, OUTPUT_SEQUENCE_LENGTH), token_to_index['_pad_'], dtype=INTX)\n",
    "for line_idx, line in enumerate(y_data_iterator_for_context):\n",
    "    if line_idx >= max_lines_num:\n",
    "        break\n",
    "    line = ['_start_'] + line + ['_end_']\n",
    "    for token_idx, token in enumerate(line[:max_line_len]):\n",
    "        Y[line_idx, token_idx] = token_to_index[token] if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "# condition数据转成ID\n",
    "y_conditions_iterator = islice(train_conditions, 1, None, 2)\n",
    "condition_ids_iterator = map(lambda condition: condition_to_index.get(condition, condition_to_index['neutral']), \\\n",
    "                             y_conditions_iterator)\n",
    "condition_ids = np.full(n_dialogs, condition_to_index['neutral'], dtype=INTX)\n",
    "for sample_idx, condition_id in enumerate(condition_ids_iterator):\n",
    "    condition_ids[sample_idx] = condition_id\n",
    "    \n",
    "x_train = X\n",
    "y_train = Y\n",
    "condition_ids_train = condition_ids\n",
    "\n",
    "print (list(x_train)[:3])\n",
    "print (list(y_train)[:3])\n",
    "print (list(condition_ids_train)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成上下文无关的验证集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint16), array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [12, 13, 14, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16), array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [12,  1, 13, 14, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16)]\n",
      "[array([2, 8, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint16), array([2, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint16), array([2, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint16)]\n"
     ]
    }
   ],
   "source": [
    "MAX_VAL_LINES_NUM = 10000\n",
    "\n",
    "with open('data/corpora_processed/context_free_validation_set.txt', 'r', encoding='utf-8') as fh:\n",
    "    test_lines = [line.strip() for line in fh.readlines()]\n",
    "    test_lines = list(filter(None, test_lines))\n",
    "tokenized_validation_lines = []\n",
    "tokens_voc = set(token_to_index.keys())\n",
    "for line in test_lines:\n",
    "    tokenized_line = _tokenizer.tokenize(line)\n",
    "    tokenized_line = [t if t in tokens_voc else '_unk_' for t in tokenized_line]\n",
    "    tokenized_validation_lines.append(tokenized_line)\n",
    "tokenized_validation_lines = tokenized_validation_lines[:MAX_VAL_LINES_NUM]\n",
    "\n",
    "# 数据进行X、Y的区分\n",
    "n_dialogs = sum(1 for _ in tokenized_validation_lines)\n",
    "x_data_iterator_seq2seq = islice(tokenized_validation_lines, 0, None, 2)\n",
    "y_data_iterator_seq2seq = islice(tokenized_validation_lines, 1, None, 2)\n",
    "n_dialogs //= 2\n",
    "context = []\n",
    "x_data_iterator = [] # 验证的输入x\n",
    "y_data_iterator_for_context = [] # 验证的输入y\n",
    "last_y_line = None\n",
    "for x_line, y_line in zip(x_data_iterator_seq2seq, y_data_iterator_seq2seq):\n",
    "    if x_line != last_y_line:\n",
    "        context = []  # clear context if last response != current dialog context (new dialog)\n",
    "    context.append(x_line)\n",
    "    x_data_iterator.append(context[-INPUT_CONTEXT_SIZE:])  # yield list of tokenized lines\n",
    "    y_data_iterator_for_context.append(y_line)\n",
    "    last_y_line = y_line\n",
    "\n",
    "# X数据转成ID\n",
    "max_contexts_num = n_dialogs\n",
    "X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index['_pad_'], dtype=INTX)\n",
    "for context_idx, context in enumerate(x_data_iterator):\n",
    "    if context_idx >= max_contexts_num:\n",
    "        break\n",
    "    # take last max_content_len utterances\n",
    "    context = context[-max_context_len:]\n",
    "    # fill utterances to the end of context, keep first empty utterances padded.\n",
    "    utterance_offset = max_context_len - len(context)\n",
    "    for utterance_idx, utterance in enumerate(context):\n",
    "        for token_idx, token in enumerate(utterance[:max_line_len]):\n",
    "            X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n",
    "                if token in token_to_index else token_to_index['_unk_']\n",
    "                \n",
    "# Y数据转成ID\n",
    "max_lines_num = n_dialogs\n",
    "Y = np.full((max_lines_num, OUTPUT_SEQUENCE_LENGTH), token_to_index['_pad_'], dtype=INTX)\n",
    "for line_idx, line in enumerate(y_data_iterator_for_context):\n",
    "    if line_idx >= max_lines_num:\n",
    "        break\n",
    "    line = ['_start_'] + line + ['_end_']\n",
    "    for token_idx, token in enumerate(line[:max_line_len]):\n",
    "        Y[line_idx, token_idx] = token_to_index[token] if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "x_validation_free = X\n",
    "y_validation_free = Y\n",
    "condition_ids_validation_free = None\n",
    "\n",
    "print (list(x_validation_free)[:3])\n",
    "print (list(y_validation_free)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成上下文相关的验证集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5,  1,  5,  1,  1,  1,  1, 17,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16), array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5,  1,  5,  1,  1,  1,  1, 17,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  5,  1,  1, 14, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16), array([[ 1,  5,  1,  5,  1,  1,  1,  1, 17,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  5,  1,  1, 14, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5,  1,  4,  1, 14,  1,  1, 23,  1,  1, 17,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "      dtype=uint16)]\n",
      "[array([ 2,  1,  1,  5,  1,  1, 14, 17,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=uint16), array([ 2,  1,  5,  1,  4,  1, 14,  1,  1, 23,  1,  1, 17,  3,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=uint16), array([ 2,  1,  5,  1,  1,  1,  1, 23,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        9,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=uint16)]\n"
     ]
    }
   ],
   "source": [
    "# 获取对话数据\n",
    "context_sensitive_val_dialogs = []\n",
    "tokenized_valid_lines = []\n",
    "for line in open('data/corpora_processed/val_processed_dialogs.txt', 'r', encoding='utf-8'):\n",
    "    line_json = json.loads(line.strip())\n",
    "    dias = []\n",
    "    for entry in line_json:\n",
    "        tokens = _tokenizer.tokenize(entry['text'])\n",
    "        tokenized_valid_lines.append(tokens)\n",
    "        dias.append({'text': ' '.join(tokens), 'condition': entry['condition']})\n",
    "    context_sensitive_val_dialogs.append(dias)\n",
    "context_sensitive_val_dialogs = islice(context_sensitive_val_dialogs, MAX_VAL_LINES_NUM)\n",
    "    \n",
    "# 分别拿到对话和condition\n",
    "tokenized_alternated_context_sensitive_val_lines = []\n",
    "alternated_context_sensitive_val_conditions = []\n",
    "for dialog in context_sensitive_val_dialogs:\n",
    "    for first_dialog_line, second_dialog_line in zip(dialog, dialog[1:]):\n",
    "        tokenized_alternated_context_sensitive_val_lines.append(_tokenizer.tokenize(first_dialog_line['text']))\n",
    "        tokenized_alternated_context_sensitive_val_lines.append(_tokenizer.tokenize(second_dialog_line['text']))\n",
    "        alternated_context_sensitive_val_conditions.append(first_dialog_line['condition'])\n",
    "    alternated_context_sensitive_val_conditions.append(second_dialog_line['condition'])\n",
    "\n",
    "# x_context_sensitive_val, y_context_sensitive_val, num_context_sensitive_val_dialogs = \\\n",
    "#         transform_lines_to_nn_input(tokenized_alternated_context_sensitive_val_lines, token_to_index)\n",
    "\n",
    "# 数据进行X、Y的区分\n",
    "n_dialogs = sum(1 for _ in tokenized_alternated_context_sensitive_val_lines)\n",
    "x_data_iterator_seq2seq = islice(tokenized_alternated_context_sensitive_val_lines, 0, None, 2)\n",
    "y_data_iterator_seq2seq = islice(tokenized_alternated_context_sensitive_val_lines, 1, None, 2)\n",
    "n_dialogs //= 2\n",
    "context = []\n",
    "x_data_iterator = [] # 验证的输入x\n",
    "y_data_iterator_for_context = [] # 验证的输入y\n",
    "last_y_line = None\n",
    "for x_line, y_line in zip(x_data_iterator_seq2seq, y_data_iterator_seq2seq):\n",
    "    if x_line != last_y_line:\n",
    "        context = []  # clear context if last response != current dialog context (new dialog)\n",
    "    context.append(x_line)\n",
    "    x_data_iterator.append(context[-INPUT_CONTEXT_SIZE:])  # yield list of tokenized lines\n",
    "    y_data_iterator_for_context.append(y_line)\n",
    "    last_y_line = y_line\n",
    "\n",
    "# X数据转成ID\n",
    "max_contexts_num = n_dialogs\n",
    "X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index['_pad_'], dtype=INTX)\n",
    "for context_idx, context in enumerate(x_data_iterator):\n",
    "    if context_idx >= max_contexts_num:\n",
    "        break\n",
    "    # take last max_content_len utterances\n",
    "    context = context[-max_context_len:]\n",
    "    # fill utterances to the end of context, keep first empty utterances padded.\n",
    "    utterance_offset = max_context_len - len(context)\n",
    "    for utterance_idx, utterance in enumerate(context):\n",
    "        for token_idx, token in enumerate(utterance[:max_line_len]):\n",
    "            X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n",
    "                if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "# Y数据转成ID\n",
    "max_lines_num = n_dialogs\n",
    "Y = np.full((max_lines_num, OUTPUT_SEQUENCE_LENGTH), token_to_index['_pad_'], dtype=INTX)\n",
    "for line_idx, line in enumerate(y_data_iterator_for_context):\n",
    "    if line_idx >= max_lines_num:\n",
    "        break\n",
    "    line = ['_start_'] + line + ['_end_']\n",
    "    for token_idx, token in enumerate(line[:max_line_len]):\n",
    "        Y[line_idx, token_idx] = token_to_index[token] if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "# condition数据转成ID\n",
    "y_conditions_iterator = islice(alternated_context_sensitive_val_conditions, 1, None, 2)\n",
    "condition_ids_iterator = map(lambda condition: condition_to_index.get(condition, condition_to_index['neutral']), \\\n",
    "                             y_conditions_iterator)\n",
    "condition_ids = np.full(n_dialogs, condition_to_index['neutral'], dtype=INTX)\n",
    "for sample_idx, condition_id in enumerate(condition_ids_iterator):\n",
    "    condition_ids[sample_idx] = condition_id\n",
    "\n",
    "x_validation_sensitive = X\n",
    "y_validation_sensitive = Y\n",
    "condition_ids_validation_sensitive = None\n",
    "\n",
    "print (list(x_validation_sensitive)[:3])\n",
    "print (list(y_validation_sensitive)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把训练数据和验证数据放入Dataset和ModelParam中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_CORPUS_NAME = 'train_processed_dialogs'\n",
    "\n",
    "train_dataset = Dataset(x=x_train, y=y_train, condition_ids=condition_ids_train)\n",
    "training_data_param = ModelParam(value=train_dataset, id=TRAIN_CORPUS_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型定义\n",
    "\n",
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from keras.layers import GRU\n",
    "    \n",
    "EPOCHS_NUM = 2\n",
    "BATCH_SIZE = 196\n",
    "MODEL_NAME = 'hred_eng_gru_v1'\n",
    "TRAIN_WORD_EMBEDDINGS_LAYER = True # 允许在模型训练过程中微调embedding\n",
    "CONDITION_EMBEDDING_DIMENSION = 128\n",
    "\n",
    "# 每个epoch要跑的batch数\n",
    "batches_num_per_epoch = math.ceil(x_train.shape[0] / BATCH_SIZE)\n",
    "\n",
    "_model_name = MODEL_NAME\n",
    "_rnn_class = partial(GRU, reset_after=True) #如果在gpu上可以使用_rnn_class=CuDNNGRU\n",
    "_index_to_token = index_to_token\n",
    "_token_to_index = {v: k for k, v in index_to_token.items()}\n",
    "_vocab_size = len(_index_to_token)\n",
    "_skip_token_id = _token_to_index['_pad_']\n",
    "_token_embedding_dim = WORD_EMBEDDING_DIMENSION\n",
    "_train_token_embedding = TRAIN_WORD_EMBEDDINGS_LAYER\n",
    "\n",
    "# 初始化embedding\n",
    "_W_init_embedding = np.zeros((len(_token_to_index), _token_embedding_dim))\n",
    "for token, index in _token_to_index.items():\n",
    "    if token in word2vec_model:\n",
    "        _W_init_embedding[index] = np.array(word2vec_model[token])\n",
    "    eif token != '_pad_':\n",
    "        _W_init_embedding[index] = numpy.random.uniform(token_embedding_dim, dtype=np.float32)\n",
    "\n",
    "_index_to_condition = index_to_condition\n",
    "_condition_to_index = {v: k for k, v in index_to_condition.items()}\n",
    "_condition_embedding_dim = CONDITION_EMBEDDING_DIMENSION\n",
    "_training_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "_PICKLE_PROTOCOL = 2\n",
    "AUTOENCODER_MODE = False\n",
    "\n",
    "def _pickle_iterable(filename, iterable):\n",
    "    with open(filename, 'wb') as pickle_fh:\n",
    "        pklr = pickle.Pickler(pickle_fh, _PICKLE_PROTOCOL)\n",
    "        for entry in iterable:\n",
    "            pklr.dump(entry)\n",
    "            pklr.clear_memo()\n",
    "\n",
    "def _open_pickle(filename):\n",
    "    return open(filename, 'rb')\n",
    "\n",
    "def _unpickle_iterable(pickle_fh):\n",
    "    with pickle_fh:\n",
    "        unpklr = pickle.Unpickler(pickle_fh)\n",
    "        try:\n",
    "            while True:\n",
    "                yield unpklr.load()\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "def file_buffered_tee(iterable, n=2):\n",
    "    _, filename = tempfile.mkstemp()\n",
    "    try:\n",
    "        _pickle_iterable(filename, iterable)\n",
    "        return tuple(_unpickle_iterable(_open_pickle(filename)) for _ in range(n))\n",
    "    finally:\n",
    "        os.remove(filename)\n",
    "\n",
    "def _get_x_data_iterator_with_context(x_data_iterator, y_data_iterator, context_size=INPUT_CONTEXT_SIZE):\n",
    "    context = []\n",
    "\n",
    "    last_y_line = None\n",
    "    for x_line, y_line in zip(x_data_iterator, y_data_iterator):\n",
    "        if x_line != last_y_line:\n",
    "            context = []  # clear context if last response != current dialog context (new dialog)\n",
    "\n",
    "        context.append(x_line)\n",
    "        yield context[-context_size:]  # yield list of tokenized lines\n",
    "        last_y_line = y_line\n",
    "def transform_contexts_to_token_ids(tokenized_contexts,\n",
    "                                    token_to_index,\n",
    "                                    max_line_len,\n",
    "                                    max_context_len=1,\n",
    "                                    max_contexts_num=None,\n",
    "                                    add_start_end=False):\n",
    "    \"\"\"\n",
    "    Transforms contexts of lines of text to matrix of indices of tokens to be used in training/predicting.\n",
    "    Uses only first max_lines_num lines of tokenized_lines. Also clips each line to max_line_len tokens.\n",
    "    if length of a line is less that max_line_len, it's padded with token_to_index[PAD_TOKEN].\n",
    "\n",
    "    :param tokenized_contexts: iterable of lists (contexts) of lists (utterances) of tokens to transform to ids\n",
    "    :param token_to_index: dict that maps each token to its id\n",
    "    :param max_line_len: maximum number of tokens in a line\n",
    "    :param max_context_len: maximum context length\n",
    "    :param max_contexts_num: maximum number of contexts\n",
    "    :param add_start_end: add start/end tokens to sequence\n",
    "    :return: X -- numpy array, dtype=INTX, shape = (max_lines_num, max_context_len, max_line_len).\n",
    "    \"\"\"\n",
    "\n",
    "    if max_contexts_num is None:\n",
    "        if not isinstance(tokenized_contexts, list):\n",
    "            raise TypeError('tokenized_lines should has list type if max_lines_num is not specified')\n",
    "        max_contexts_num = len(tokenized_contexts)\n",
    "\n",
    "    X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index['_pad_'], dtype=INTX)\n",
    "\n",
    "    for context_idx, context in enumerate(tokenized_contexts):\n",
    "        if context_idx >= max_contexts_num:\n",
    "            break\n",
    "\n",
    "        # take last max_content_len utterances\n",
    "        context = context[-max_context_len:]\n",
    "\n",
    "        # fill utterances to the end of context, keep first empty utterances padded.\n",
    "        utterance_offset = max_context_len - len(context)\n",
    "        for utterance_idx, utterance in enumerate(context):\n",
    "            if add_start_end:\n",
    "                utterance = ['_start_'] + utterance + ['_end_']\n",
    "\n",
    "            for token_idx, token in enumerate(utterance[:max_line_len]):\n",
    "                X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n",
    "                    if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "    return X\n",
    "\n",
    "def transform_lines_to_token_ids(tokenized_lines, token_to_index, max_line_len, max_lines_num=None,\n",
    "                                 add_start_end=False):\n",
    "    \"\"\"\n",
    "    Transforms lines of text to matrix of indices of tokens to be used in training/predicting.\n",
    "    Uses only first max_lines_num lines of tokenized_lines. Also clips each line to max_line_len tokens.\n",
    "    if length of a line is less that max_line_len, it's padded with token_to_index[PAD_TOKEN].\n",
    "\n",
    "    :param tokenized_lines: iterable of lists (utterances) of tokens to transform to ids\n",
    "    :param token_to_index: dict that maps each token to its id\n",
    "    :param max_line_len: maximum number of tokens in a lineh\n",
    "    :param max_lines_num: maximum number of lines\n",
    "    :param add_start_end: add start/end tokens to sequence\n",
    "    :return: X -- numpy array, dtype=INTX, shape = (max_lines_num, max_line_len).\n",
    "    \"\"\"\n",
    "\n",
    "    if max_lines_num is None:\n",
    "        if not isinstance(tokenized_lines, list):\n",
    "            raise TypeError('tokenized_lines should has list type if max_lines_num is not specified')\n",
    "        max_lines_num = len(tokenized_lines)\n",
    "\n",
    "    X = np.full((max_lines_num, max_line_len), token_to_index['_pad_'], dtype=INTX)\n",
    "\n",
    "    for line_idx, line in enumerate(tokenized_lines):\n",
    "        if line_idx >= max_lines_num:\n",
    "            break\n",
    "\n",
    "        if add_start_end:\n",
    "            line = ['_start_'] + line + ['_end_']\n",
    "\n",
    "        for token_idx, token in enumerate(line[:max_line_len]):\n",
    "            X[line_idx, token_idx] = token_to_index[token] \\\n",
    "                if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "    return X\n",
    "\n",
    "def transform_lines_to_nn_input(tokenized_dialog_lines, token_to_index, autoencoder_mode=AUTOENCODER_MODE):\n",
    "    \"\"\"\n",
    "    Splits lines (IterableSentences) and generates numpy arrays of token ids suitable for training.\n",
    "    Doesn't store all lines in memory.\n",
    "    \"\"\"\n",
    "    x_data_iterator, y_data_iterator, iterator_for_len_calc = file_buffered_tee(tokenized_dialog_lines, 3)\n",
    "    print ('Iterating through lines to get number of elements in the dataset')\n",
    "    n_dialogs = sum(1 for _ in iterator_for_len_calc)\n",
    "    print (n_dialogs)\n",
    "\n",
    "    if not autoencoder_mode:\n",
    "        # seq2seq mode\n",
    "        x_data_iterator = islice(x_data_iterator, 0, None, 2)\n",
    "        y_data_iterator = islice(y_data_iterator, 1, None, 2)\n",
    "        n_dialogs //= 2\n",
    "\n",
    "    y_data_iterator, y_data_iterator_for_context = file_buffered_tee(y_data_iterator)\n",
    "    x_data_iterator = _get_x_data_iterator_with_context(x_data_iterator, y_data_iterator_for_context)\n",
    "#     print (list(x_data_iterator))\n",
    "    print ('Iterating through lines to get input matrix')\n",
    "    x_ids = transform_contexts_to_token_ids(\n",
    "        x_data_iterator, token_to_index, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, max_contexts_num=n_dialogs)\n",
    "#     print (x_ids)\n",
    "\n",
    "    print ('Iterating through lines to get output matrix')\n",
    "    y_ids = transform_lines_to_token_ids(\n",
    "        y_data_iterator, token_to_index, OUTPUT_SEQUENCE_LENGTH, n_dialogs, add_start_end=True)\n",
    "    print (y_ids)\n",
    "    return x_ids, y_ids, n_dialogs\n",
    "\n",
    "# x_train, y_train, _ = transform_lines_to_nn_input(tokenized_alternated_train_lines, token_to_index)\n",
    "x_validation, y_validation, _ = transform_lines_to_nn_input(tokenized_validation_lines, token_to_index)\n",
    "# print (list(x_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hred)",
   "language": "python",
   "name": "hred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
