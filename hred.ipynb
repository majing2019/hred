{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据准备（data/corpora_processed、data/conditions_index）\n",
    "\n",
    "构建单词索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '_pad_'), (1, '_unk_'), (2, '_start_'), (3, '_end_'), (4, ','), (5, 'Hello'), (6, 'Oh'), (7, 'hi'), (8, '!'), (9, ':')]\n",
      "[(0, 'neutral'), (1, 'joy'), (2, 'sadness')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "\n",
    "VOCABULARY_MAX_SIZE = 50000\n",
    "MAX_CONDITIONS_NUM = 5\n",
    "\n",
    "dialogs = []\n",
    "tokens_counter = Counter()\n",
    "conditions_counter = Counter()\n",
    "tokenized_training_lines = []\n",
    "for line in open('data/corpora_processed/train_processed_dialogs.txt', 'r', encoding='utf-8'):\n",
    "    line_json = json.loads(line.strip())\n",
    "    dias = []\n",
    "    for entry in line_json:\n",
    "        tokens = nltk.word_tokenize(entry['text'])\n",
    "        tokenized_training_lines.append(tokens)\n",
    "        dias.append({'text': ' '.join(tokens), 'condition': entry['condition']})\n",
    "        tokens_counter.update(tokens)\n",
    "        conditions_counter[entry['condition']] += 1\n",
    "        \n",
    "# 构建vocab list\n",
    "special_tokens = ['_pad_', '_unk_', '_start_', '_end_']\n",
    "vocab = special_tokens + [token for token, _ in tokens_counter.most_common(VOCABULARY_MAX_SIZE - len(special_tokens))]\n",
    "\n",
    "# 构建condition list\n",
    "conditions = [condition for condition, _ in conditions_counter.most_common(MAX_CONDITIONS_NUM)]\n",
    "\n",
    "index_to_token = dict(enumerate(vocab))\n",
    "index_to_condition = dict(enumerate(conditions))\n",
    "\n",
    "with open('data/id2vocab', 'w', encoding='utf-8') as fh:\n",
    "        json.dump(index_to_token, fh, ensure_ascii=False)\n",
    "\n",
    "with open('data/id2condition', 'w', encoding='utf-8') as fh:\n",
    "        json.dump(index_to_condition, fh, ensure_ascii=False)\n",
    "        \n",
    "print(list(index_to_token.items())[:10])\n",
    "print(list(index_to_condition.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majing/workspace/learning/hred/conda-env/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "WORD_EMBEDDING_DIMENSION = 128\n",
    "W2V_WINDOW_SIZE = 10\n",
    "USE_SKIP_GRAM = True # 为False时用CBOW\n",
    "MIN_WORD_FREQ = 1\n",
    "_WORKERS_NUM = multiprocessing.cpu_count()\n",
    "\n",
    "word2vec_path = 'data/word2vec.bin'\n",
    "word2vec_model = Word2Vec(\n",
    "        window=W2V_WINDOW_SIZE,\n",
    "        size=WORD_EMBEDDING_DIMENSION,\n",
    "        max_vocab_size=VOCABULARY_MAX_SIZE,\n",
    "        min_count=MIN_WORD_FREQ,\n",
    "        workers=_WORKERS_NUM,\n",
    "        sg=USE_SKIP_GRAM)\n",
    "\n",
    "word2vec_model.build_vocab(tokenized_training_lines)\n",
    "word2vec_model.train(tokenized_training_lines, total_words=50000, epochs=10)\n",
    "word2vec_model.init_sims(replace=True) # 强制单位归一化，破坏性就地(打击非归一化向量), 更节省存储空间\n",
    "word2vec_model.save(word2vec_path, separately=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(42)\n",
    "numpy.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入vocab和condition索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_pad_': 0, '_unk_': 1, '_start_': 2, '_end_': 3, ',': 4, 'Hello': 5, 'Oh': 6, 'hi': 7, '!': 8, ':': 9, ')': 10, 'How': 11, 'are': 12, 'you': 13, 'my': 14, 'friend': 15, '?': 16, 'Doing': 17, 'good': 18, 'Justin': 19, 'Bieber': 20, 'is': 21, 'the': 22, 'best': 23, 'Ok': 24, '...': 25}\n",
      "{'neutral': 0, 'joy': 1, 'sadness': 2}\n"
     ]
    }
   ],
   "source": [
    "with open('data/id2vocab', 'r', encoding='utf-8') as item_index_fh:\n",
    "        token_to_index = json.load(item_index_fh)\n",
    "        token_to_index = {v: int(k) for k, v in token_to_index.items()}\n",
    "\n",
    "\n",
    "with open('data/id2condition', 'r', encoding='utf-8') as item_index_fh:\n",
    "        condition_to_index = json.load(item_index_fh)\n",
    "        condition_to_index = {v: int(k) for k, v in condition_to_index.items()}\n",
    "        \n",
    "print (token_to_index)\n",
    "print (condition_to_index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through lines to get number of elements in the dataset\n",
      "==================\n",
      "[['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.'], ['oh', ',', 'hi', '!', ':', ')', 'how', 'are', 'you', ',', 'my', 'friend', '?'], ['doing', 'good'], ['ok', '.', '.', '.']]\n",
      "Iterating through lines to get input matrix\n",
      "Iterating through lines to get output matrix\n",
      "Iterating through conditions of output list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(x=array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint16), y=array([[ 2,  1,  4, ...,  0,  0,  0],\n",
       "       [ 2,  1, 18, ...,  0,  0,  0],\n",
       "       [ 2,  1,  1, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 2,  1,  4, ...,  0,  0,  0],\n",
       "       [ 2,  1, 18, ...,  0,  0,  0],\n",
       "       [ 2,  1,  1, ...,  0,  0,  0]], dtype=uint16), condition_ids=array([1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1,\n",
       "       0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0,\n",
       "       2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "       1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1,\n",
       "       0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0,\n",
       "       2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "       1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2], dtype=uint16))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "import tempfile\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def create_namedtuple_instance(name, **kwargs):\n",
    "    return collections.namedtuple(name, kwargs.keys())(**kwargs)\n",
    "            \n",
    "INPUT_SEQUENCE_LENGTH = 30\n",
    "AUTOENCODER_MODE = False\n",
    "INPUT_CONTEXT_SIZE = 3\n",
    "_PICKLE_PROTOCOL = 2\n",
    "INTX = 'uint16'\n",
    "OUTPUT_SEQUENCE_LENGTH = 32\n",
    "EMOTIONS_TYPES = create_namedtuple_instance(\n",
    "    'EMOTIONS_TYPES', neutral='neutral', anger='anger', joy='joy', fear='fear', sadness='sadness')\n",
    "DEFAULT_CONDITION = EMOTIONS_TYPES.neutral\n",
    "Dataset = namedtuple('Dataset', ['x', 'y', 'condition_ids'])\n",
    "\n",
    "SPECIAL_TOKENS = create_namedtuple_instance(\n",
    "    'SPECIAL_TOKENS', PAD_TOKEN='_pad_', UNKNOWN_TOKEN='_unk_', START_TOKEN='_start_', EOS_TOKEN='_end_')\n",
    "_tokenizer = nltk.tokenize.RegexpTokenizer(pattern='\\w+|[^\\w\\s]')\n",
    "\n",
    "def _pickle_iterable(filename, iterable):\n",
    "    with open(filename, 'wb') as pickle_fh:\n",
    "        pklr = pickle.Pickler(pickle_fh, _PICKLE_PROTOCOL)\n",
    "        for entry in iterable:\n",
    "            pklr.dump(entry)\n",
    "            pklr.clear_memo()\n",
    "          \n",
    "def _unpickle_iterable(pickle_fh):\n",
    "    with pickle_fh:\n",
    "        unpklr = pickle.Unpickler(pickle_fh)\n",
    "        try:\n",
    "            while True:\n",
    "                yield unpklr.load()\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "def _open_pickle(filename):\n",
    "    return open(filename, 'rb')\n",
    "        \n",
    "class FileTextLinesIterator(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self._filename, 'r', encoding='utf-8'):\n",
    "            yield line.strip()\n",
    "\n",
    "    def __copy__(self):\n",
    "        return FileTextLinesIterator(self._filename)\n",
    "    \n",
    "class ProcessedLinesIterator(object):\n",
    "    def __init__(self, lines_iter, processing_callbacks=None):\n",
    "        self._lines_iter = lines_iter\n",
    "        self._processing_callbacks = processing_callbacks if processing_callbacks else []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in self._lines_iter:\n",
    "            for callback in self._processing_callbacks:\n",
    "                line = callback(line)\n",
    "            yield line\n",
    "\n",
    "    def __copy__(self):\n",
    "        return ProcessedLinesIterator(copy(self._lines_iter), self._processing_callbacks)\n",
    "\n",
    "class JsonTextLinesIterator(object):\n",
    "    def __init__(self, text_lines_iter):\n",
    "        self._text_lines_iter = text_lines_iter\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in self._text_lines_iter:\n",
    "            try:\n",
    "                yield json.loads(line.strip())\n",
    "            except ValueError:\n",
    "                print ('Skipped invalid json object: \"{}\"'.format(line.strip()))\n",
    "                continue\n",
    "\n",
    "class itemgetter:\n",
    "    \"\"\"\n",
    "    Return a callable object that fetches the given item(s) from its operand.\n",
    "    After f = itemgetter(2), the call f(r) returns r[2].\n",
    "    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
    "    \"\"\"\n",
    "    __slots__ = ('_items', '_call')\n",
    "\n",
    "    def __init__(self, item, *items):\n",
    "        if not items:\n",
    "            self._items = (item,)\n",
    "            def func(obj):\n",
    "                return obj[item]\n",
    "            self._call = func\n",
    "        else:\n",
    "            self._items = items = (item,) + items\n",
    "            def func(obj):\n",
    "                return tuple(obj[i] for i in items)\n",
    "            self._call = func\n",
    "\n",
    "    def __call__(self, obj):\n",
    "        return self._call(obj)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
    "                              self.__class__.__name__,\n",
    "                              ', '.join(map(repr, self._items)))\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, self._items\n",
    "    \n",
    "def _get_x_data_iterator_with_context(x_data_iterator, y_data_iterator, context_size=INPUT_CONTEXT_SIZE):\n",
    "    context = []\n",
    "\n",
    "    last_y_line = None\n",
    "    for x_line, y_line in zip(x_data_iterator, y_data_iterator):\n",
    "        if x_line != last_y_line:\n",
    "            context = []  # clear context if last response != current dialog context (new dialog)\n",
    "\n",
    "        context.append(x_line)\n",
    "        yield context[-context_size:]  # yield list of tokenized lines\n",
    "        last_y_line = y_line\n",
    "        \n",
    "def transform_lines_to_nn_input(tokenized_dialog_lines, token_to_index, autoencoder_mode=AUTOENCODER_MODE):\n",
    "    \"\"\"\n",
    "    Splits lines (IterableSentences) and generates numpy arrays of token ids suitable for training.\n",
    "    Doesn't store all lines in memory.\n",
    "    \"\"\"\n",
    "    x_data_iterator, y_data_iterator, iterator_for_len_calc = file_buffered_tee(tokenized_dialog_lines, 3)\n",
    "\n",
    "#     print ('==================')\n",
    "#     print (list(y_data_iterator))\n",
    "    \n",
    "    print ('Iterating through lines to get number of elements in the dataset')\n",
    "    n_dialogs = sum(1 for _ in iterator_for_len_calc)\n",
    "\n",
    "    if not autoencoder_mode:\n",
    "        # seq2seq mode\n",
    "        x_data_iterator = islice(x_data_iterator, 0, None, 2)\n",
    "        y_data_iterator = islice(y_data_iterator, 1, None, 2)\n",
    "        n_dialogs //= 2\n",
    "\n",
    "    y_data_iterator, y_data_iterator_for_context = file_buffered_tee(y_data_iterator)\n",
    "#     print ('==================')\n",
    "#     print (list(y_data_iterator_for_context))\n",
    "    x_data_iterator = _get_x_data_iterator_with_context(x_data_iterator, y_data_iterator_for_context)\n",
    "#     print ('================')\n",
    "#     print (list(x_data_iterator))\n",
    "    \n",
    "    print ('Iterating through lines to get input matrix')\n",
    "    x_ids = transform_contexts_to_token_ids(\n",
    "        x_data_iterator, token_to_index, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, max_contexts_num=n_dialogs)\n",
    "\n",
    "    print ('Iterating through lines to get output matrix')\n",
    "    y_ids = transform_lines_to_token_ids(\n",
    "        y_data_iterator, token_to_index, OUTPUT_SEQUENCE_LENGTH, n_dialogs, add_start_end=True)\n",
    "    return x_ids, y_ids, n_dialogs\n",
    "\n",
    "def get_tokens_sequence(text, lower=True, check_unicode=True):\n",
    "    if check_unicode and not isinstance(text, str):\n",
    "        raise TypeError('Text object should be unicode type. Got instead \"{}\" of type {}'.format(text, type(text)))\n",
    "\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    tokens = _tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def transform_lines_to_token_ids(tokenized_lines, token_to_index, max_line_len, max_lines_num=None,\n",
    "                                 add_start_end=False):\n",
    "    \"\"\"\n",
    "    Transforms lines of text to matrix of indices of tokens to be used in training/predicting.\n",
    "    Uses only first max_lines_num lines of tokenized_lines. Also clips each line to max_line_len tokens.\n",
    "    if length of a line is less that max_line_len, it's padded with token_to_index[PAD_TOKEN].\n",
    "\n",
    "    :param tokenized_lines: iterable of lists (utterances) of tokens to transform to ids\n",
    "    :param token_to_index: dict that maps each token to its id\n",
    "    :param max_line_len: maximum number of tokens in a lineh\n",
    "    :param max_lines_num: maximum number of lines\n",
    "    :param add_start_end: add start/end tokens to sequence\n",
    "    :return: X -- numpy array, dtype=INTX, shape = (max_lines_num, max_line_len).\n",
    "    \"\"\"\n",
    "\n",
    "    if max_lines_num is None:\n",
    "        if not isinstance(tokenized_lines, list):\n",
    "            raise TypeError('tokenized_lines should has list type if max_lines_num is not specified')\n",
    "        max_lines_num = len(tokenized_lines)\n",
    "\n",
    "    X = np.full((max_lines_num, max_line_len), token_to_index[SPECIAL_TOKENS.PAD_TOKEN], dtype=INTX)\n",
    "\n",
    "    for line_idx, line in enumerate(tokenized_lines):\n",
    "        if line_idx >= max_lines_num:\n",
    "            break\n",
    "\n",
    "        if add_start_end:\n",
    "            line = [SPECIAL_TOKENS.START_TOKEN] + line + [SPECIAL_TOKENS.EOS_TOKEN]\n",
    "\n",
    "        for token_idx, token in enumerate(line[:max_line_len]):\n",
    "            X[line_idx, token_idx] = token_to_index[token] \\\n",
    "                if token in token_to_index else token_to_index[SPECIAL_TOKENS.UNKNOWN_TOKEN]\n",
    "\n",
    "    return X\n",
    "\n",
    "def transform_contexts_to_token_ids(tokenized_contexts,\n",
    "                                    token_to_index,\n",
    "                                    max_line_len,\n",
    "                                    max_context_len=1,\n",
    "                                    max_contexts_num=None,\n",
    "                                    add_start_end=False):\n",
    "    \"\"\"\n",
    "    Transforms contexts of lines of text to matrix of indices of tokens to be used in training/predicting.\n",
    "    Uses only first max_lines_num lines of tokenized_lines. Also clips each line to max_line_len tokens.\n",
    "    if length of a line is less that max_line_len, it's padded with token_to_index[PAD_TOKEN].\n",
    "\n",
    "    :param tokenized_contexts: iterable of lists (contexts) of lists (utterances) of tokens to transform to ids\n",
    "    :param token_to_index: dict that maps each token to its id\n",
    "    :param max_line_len: maximum number of tokens in a line\n",
    "    :param max_context_len: maximum context length\n",
    "    :param max_contexts_num: maximum number of contexts\n",
    "    :param add_start_end: add start/end tokens to sequence\n",
    "    :return: X -- numpy array, dtype=INTX, shape = (max_lines_num, max_context_len, max_line_len).\n",
    "    \"\"\"\n",
    "\n",
    "    if max_contexts_num is None:\n",
    "        if not isinstance(tokenized_contexts, list):\n",
    "            raise TypeError('tokenized_lines should has list type if max_lines_num is not specified')\n",
    "        max_contexts_num = len(tokenized_contexts)\n",
    "\n",
    "    X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index[SPECIAL_TOKENS.PAD_TOKEN], dtype=INTX)\n",
    "    \n",
    "    for context_idx, context in enumerate(tokenized_contexts):\n",
    "        if context_idx >= max_contexts_num:\n",
    "            break\n",
    "\n",
    "        # take last max_content_len utterances\n",
    "        context = context[-max_context_len:]\n",
    "\n",
    "        # fill utterances to the end of context, keep first empty utterances padded.\n",
    "        utterance_offset = max_context_len - len(context)\n",
    "        for utterance_idx, utterance in enumerate(context):\n",
    "            if add_start_end:\n",
    "                utterance = [SPECIAL_TOKENS.START_TOKEN] + utterance + [SPECIAL_TOKENS.EOS_TOKEN]\n",
    "\n",
    "            for token_idx, token in enumerate(utterance[:max_line_len]):\n",
    "                X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n",
    "                    if token in token_to_index else token_to_index[SPECIAL_TOKENS.UNKNOWN_TOKEN]\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_processed_corpus_path():\n",
    "    return 'data/corpora_processed/train_processed_dialogs.txt'\n",
    "\n",
    "def load_processed_dialogs_from_json(lines, text_field_name, condition_field_name):\n",
    "    for line_json in JsonTextLinesIterator(lines):\n",
    "        yield [{\n",
    "            text_field_name: entry['text'],\n",
    "            condition_field_name: entry['condition']\n",
    "        } for entry in line_json]\n",
    "        \n",
    "def file_buffered_tee(iterable, n=2):\n",
    "    _, filename = tempfile.mkstemp()\n",
    "    try:\n",
    "        _pickle_iterable(filename, iterable)\n",
    "        return tuple(_unpickle_iterable(_open_pickle(filename)) for _ in range(n))\n",
    "    finally:\n",
    "        os.remove(filename)\n",
    "        \n",
    "def get_dialog_lines_and_conditions(dialog_lines, text_field_name, condition_field_name):\n",
    "    \"\"\"\n",
    "    Splits one dialog_lines generator into two generators - one for conditions and one for dialog lines\n",
    "    \"\"\"\n",
    "    conditions_iter, dialog_lines_iter = file_buffered_tee(\n",
    "        map(lambda line: [line[condition_field_name], line[text_field_name]], dialog_lines))\n",
    "    conditions_iter = map(itemgetter(0), conditions_iter)\n",
    "    dialog_lines_iter = map(itemgetter(1), dialog_lines_iter)\n",
    "    return dialog_lines_iter, conditions_iter\n",
    "\n",
    "def transform_conditions_to_nn_input(dialog_conditions, condition_to_index, num_dialogs):\n",
    "    y_conditions_iterator = islice(dialog_conditions, 1, None, 2)\n",
    "\n",
    "    print ('Iterating through conditions of output list')\n",
    "    return transform_conditions_to_ids(y_conditions_iterator, condition_to_index, num_dialogs)\n",
    "\n",
    "def get_alternated_dialogs_lines(dialogs):\n",
    "    for dialog in dialogs:\n",
    "        for first_dialog_line, second_dialog_line in zip(dialog, dialog[1:]):\n",
    "            yield first_dialog_line\n",
    "            yield second_dialog_line\n",
    "\n",
    "def transform_conditions_to_ids(conditions, condition_to_index, n_dialogs):\n",
    "    condition_ids_iterator = map(\n",
    "        lambda condition: condition_to_index.get(condition, condition_to_index[DEFAULT_CONDITION]), conditions)\n",
    "    condition_ids = np.full(n_dialogs, condition_to_index[DEFAULT_CONDITION], dtype=INTX)\n",
    "    # shape == (n_dialogs, )\n",
    "    for sample_idx, condition_id in enumerate(condition_ids_iterator):\n",
    "        condition_ids[sample_idx] = condition_id\n",
    "\n",
    "    # shape == (n_dialogs, 1)\n",
    "    return condition_ids\n",
    "\n",
    "def load_conditioned_dataset(token_to_index, condition_to_index, subset_size=None):\n",
    "    processed_corpus_path = get_processed_corpus_path()\n",
    "    dialogs = load_processed_dialogs_from_json(\n",
    "        FileTextLinesIterator(processed_corpus_path), text_field_name='text', condition_field_name='condition')\n",
    "    train_lines, train_conditions = get_dialog_lines_and_conditions(\n",
    "        get_alternated_dialogs_lines(dialogs), text_field_name='text', condition_field_name='condition')\n",
    "#     print (list(train_lines))\n",
    "    \n",
    "#     for ky in train_lines:\n",
    "#         print (ky)\n",
    "    \n",
    "    tokenized_alternated_train_lines = ProcessedLinesIterator(train_lines, processing_callbacks=[get_tokens_sequence])\n",
    "\n",
    "#     print (list(tokenized_alternated_train_lines))\n",
    "    # prepare train set\n",
    "    x_train, y_train, n_dialogs = transform_lines_to_nn_input(tokenized_alternated_train_lines, token_to_index)\n",
    "\n",
    "#     print (x_train[0][2])\n",
    "    \n",
    "    condition_ids_train = transform_conditions_to_nn_input(train_conditions, condition_to_index, n_dialogs)\n",
    "    return Dataset(x=x_train, y=y_train, condition_ids=condition_ids_train)\n",
    "\n",
    "def get_training_dataset(token_to_index,\n",
    "                         condition_to_index,\n",
    "                         is_reverse_model,\n",
    "                         train_subset_size=None):\n",
    "    train_dataset = load_conditioned_dataset(token_to_index, condition_to_index, train_subset_size)\n",
    "\n",
    "    return train_dataset\n",
    "\n",
    "get_training_dataset(token_to_index, condition_to_index, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hred)",
   "language": "python",
   "name": "hred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
