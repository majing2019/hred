{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据准备（data/corpora_processed、data/conditions_index）\n",
    "\n",
    "构建单词索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '_pad_'), (1, '_unk_'), (2, '_start_'), (3, '_end_'), (4, '.'), (5, ','), (6, 'Hello'), (7, 'Oh'), (8, 'hi'), (9, '!')]\n",
      "[(0, 'neutral'), (1, 'joy'), (2, 'sadness')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "_tokenizer = nltk.tokenize.RegexpTokenizer(pattern='\\w+|[^\\w\\s]')\n",
    "\n",
    "VOCABULARY_MAX_SIZE = 50000\n",
    "MAX_CONDITIONS_NUM = 5\n",
    "\n",
    "dialogs = []\n",
    "tokens_counter = Counter()\n",
    "conditions_counter = Counter()\n",
    "tokenized_training_lines = []\n",
    "for line in open('data/corpora_processed/train_processed_dialogs.txt', 'r', encoding='utf-8'):\n",
    "    line_json = json.loads(line.strip())\n",
    "    dias = []\n",
    "    for entry in line_json:\n",
    "        tokens = _tokenizer.tokenize(entry['text'])\n",
    "        tokenized_training_lines.append(tokens)\n",
    "        dias.append({'text': ' '.join(tokens), 'condition': entry['condition']})\n",
    "        tokens_counter.update(tokens)\n",
    "        conditions_counter[entry['condition']] += 1\n",
    "    dialogs.append(dias)\n",
    "        \n",
    "# 构建vocab list\n",
    "special_tokens = ['_pad_', '_unk_', '_start_', '_end_']\n",
    "vocab = special_tokens + [token for token, _ in tokens_counter.most_common(VOCABULARY_MAX_SIZE - len(special_tokens))]\n",
    "\n",
    "# 构建condition list\n",
    "conditions = [condition for condition, _ in conditions_counter.most_common(MAX_CONDITIONS_NUM)]\n",
    "\n",
    "index_to_token = dict(enumerate(vocab))\n",
    "index_to_condition = dict(enumerate(conditions))\n",
    "\n",
    "with open('data/id2vocab', 'w', encoding='utf-8') as fh:\n",
    "        json.dump(index_to_token, fh, ensure_ascii=False)\n",
    "\n",
    "with open('data/id2condition', 'w', encoding='utf-8') as fh:\n",
    "        json.dump(index_to_condition, fh, ensure_ascii=False)\n",
    "        \n",
    "print(list(index_to_token.items())[:10])\n",
    "print(list(index_to_condition.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majing/workspace/learning/hred/conda-env/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "WORD_EMBEDDING_DIMENSION = 128\n",
    "W2V_WINDOW_SIZE = 10\n",
    "USE_SKIP_GRAM = True # 为False时用CBOW\n",
    "MIN_WORD_FREQ = 1\n",
    "_WORKERS_NUM = multiprocessing.cpu_count()\n",
    "\n",
    "word2vec_path = 'data/word2vec.bin'\n",
    "word2vec_model = Word2Vec(\n",
    "        window=W2V_WINDOW_SIZE,\n",
    "        size=WORD_EMBEDDING_DIMENSION,\n",
    "        max_vocab_size=VOCABULARY_MAX_SIZE,\n",
    "        min_count=MIN_WORD_FREQ,\n",
    "        workers=_WORKERS_NUM,\n",
    "        sg=USE_SKIP_GRAM)\n",
    "\n",
    "word2vec_model.build_vocab(tokenized_training_lines)\n",
    "word2vec_model.train(tokenized_training_lines, total_words=50000, epochs=10)\n",
    "word2vec_model.init_sims(replace=True) # 强制单位归一化，破坏性就地(打击非归一化向量), 更节省存储空间\n",
    "word2vec_model.save(word2vec_path, separately=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入vocab和condition索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_pad_': 0, '_unk_': 1, '_start_': 2, '_end_': 3, '.': 4, ',': 5, 'Hello': 6, 'Oh': 7, 'hi': 8, '!': 9, ':': 10, ')': 11, 'How': 12, 'are': 13, 'you': 14, 'my': 15, 'friend': 16, '?': 17, 'Doing': 18, 'good': 19, 'Justin': 20, 'Bieber': 21, 'is': 22, 'the': 23, 'best': 24, 'Ok': 25}\n",
      "{'neutral': 0, 'joy': 1, 'sadness': 2}\n"
     ]
    }
   ],
   "source": [
    "with open('data/id2vocab', 'r', encoding='utf-8') as item_index_fh:\n",
    "        token_to_index = json.load(item_index_fh)\n",
    "        token_to_index = {v: int(k) for k, v in token_to_index.items()}\n",
    "\n",
    "\n",
    "with open('data/id2condition', 'r', encoding='utf-8') as item_index_fh:\n",
    "        condition_to_index = json.load(item_index_fh)\n",
    "        condition_to_index = {v: int(k) for k, v in condition_to_index.items()}\n",
    "        \n",
    "print (token_to_index)\n",
    "print (condition_to_index)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "\n",
    "INPUT_CONTEXT_SIZE = 3\n",
    "INPUT_SEQUENCE_LENGTH = 30\n",
    "INPUT_SEQUENCE_LENGTH = 30\n",
    "OUTPUT_SEQUENCE_LENGTH = 32\n",
    "INTX = 'uint16'\n",
    "\n",
    "# 对数据进行分词等操作\n",
    "train_conditions = []\n",
    "tokenized_alternated_train_lines = []\n",
    "y_data_iterator_for_context = []\n",
    "for dialog in dialogs:\n",
    "    for first_dialog_line, second_dialog_line in zip(dialog, dialog[1:]):\n",
    "        tokenized_alternated_train_lines.append(_tokenizer.tokenize(first_dialog_line['text']))\n",
    "        tokenized_alternated_train_lines.append(_tokenizer.tokenize(second_dialog_line['text']))\n",
    "        y_data_iterator_for_context.append(_tokenizer.tokenize(second_dialog_line['text']))\n",
    "        train_conditions.append(first_dialog_line['condition'])\n",
    "        train_conditions.append(second_dialog_line['condition'])\n",
    "\n",
    "# 数据进行X、Y的区分\n",
    "n_dialogs = sum(1 for _ in tokenized_alternated_train_lines)\n",
    "x_data_iterator_seq2seq = islice(tokenized_alternated_train_lines, 0, None, 2)\n",
    "context = []\n",
    "x_data_iterator = []\n",
    "last_y_line = None\n",
    "for x_line, y_line in zip(x_data_iterator_seq2seq, y_data_iterator_for_context):\n",
    "    if x_line != last_y_line:\n",
    "        context = []  # clear context if last response != current dialog context (new dialog)\n",
    "    context.append(x_line)\n",
    "    x_data_iterator.append(context[-INPUT_CONTEXT_SIZE:])  # yield list of tokenized lines\n",
    "    last_y_line = y_line\n",
    "\n",
    "# X数据转成ID\n",
    "n_dialogs = sum(1 for _ in tokenized_alternated_train_lines)\n",
    "n_dialogs //= 2\n",
    "max_contexts_num = n_dialogs\n",
    "max_context_len = INPUT_CONTEXT_SIZE\n",
    "max_line_len = INPUT_SEQUENCE_LENGTH\n",
    "X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index['_pad_'], dtype=INTX)\n",
    "for context_idx, context in enumerate(x_data_iterator):\n",
    "    if context_idx >= max_contexts_num:\n",
    "        break\n",
    "\n",
    "    # take last max_content_len utterances\n",
    "    context = context[-max_context_len:]\n",
    "\n",
    "    # fill utterances to the end of context, keep first empty utterances padded.\n",
    "    utterance_offset = max_context_len - len(context)\n",
    "    for utterance_idx, utterance in enumerate(context):\n",
    "        for token_idx, token in enumerate(utterance[:max_line_len]):\n",
    "            X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n",
    "                if token in token_to_index else token_to_index[_unk_]\n",
    "                \n",
    "# Y数据转成ID\n",
    "max_lines_num = n_dialogs\n",
    "Y = np.full((max_lines_num, OUTPUT_SEQUENCE_LENGTH), token_to_index['_pad_'], dtype=INTX)\n",
    "for line_idx, line in enumerate(y_data_iterator_for_context):\n",
    "    if line_idx >= max_lines_num:\n",
    "        break\n",
    "\n",
    "    line = ['_start_'] + line + ['_end_']\n",
    "\n",
    "    for token_idx, token in enumerate(line[:max_line_len]):\n",
    "        Y[line_idx, token_idx] = token_to_index[token] if token in token_to_index else token_to_index['_unk_']\n",
    "\n",
    "# condition数据转成ID\n",
    "y_conditions_iterator = islice(train_conditions, 1, None, 2)\n",
    "condition_ids_iterator = map(lambda condition: condition_to_index.get(condition, condition_to_index['neutral']), \\\n",
    "                             y_conditions_iterator)\n",
    "condition_ids = np.full(n_dialogs, condition_to_index['neutral'], dtype=INTX)\n",
    "for sample_idx, condition_id in enumerate(condition_ids_iterator):\n",
    "    condition_ids[sample_idx] = condition_id\n",
    "    \n",
    "x_train = X\n",
    "y_train = Y\n",
    "condition_to_index = condition_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 6  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 6  0  0 ...  0  0  0]\n",
      "  [ 7  5  8 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [20 21 22 ...  0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 6  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 6  0  0 ...  0  0  0]\n",
      "  [ 7  5  8 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [20 21 22 ...  0  0  0]]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hred)",
   "language": "python",
   "name": "hred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
